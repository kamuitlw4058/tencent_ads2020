{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from  collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "\n",
    "np.random.seed(2019)\n",
    "random.seed(2019)\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 280)\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "data_path = '/data/workspace/kimi/tencent_ads/2020/dataset'\n",
    "preprocess_path = 'preprocess'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           user_id                                                                                                                                              ad_id_seq\n",
      "0        3969503.0  [131508, 24135, 179398, 212955, 298663, 18413, 82996, 121567, 581344, 222576, 410358, 27028, 102071, 74685, 1215844, 241849, 273087, 1437928, 1187...\n",
      "1           2267.0  [223979, 139563, 79026, 220199, 220126, 274249, 190996, 461148, 50437, 274189, 220199, 533051, 163703, 107932, 113724, 761306, 192657, 624676, 355...\n",
      "2         512898.0  [150988, 133836, 150708, 246310, 250548, 306943, 320970, 204541, 87899, 107984, 147107, 140319, 569029, 598774, 290696, 683618, 690876, 170732, 53...\n",
      "...            ...                                                                                                                                                    ...\n",
      "1899997   742408.0                  [2619335, 3771589, 3332913, 3574432, 73976, 3114229, 3712996, 2495986, 3553252, 1900218, 1757244, 3770207, 2305828, 3724445, 3782579]\n",
      "1899998  3083446.0                                                                      [1727254, 3482289, 3624668, 3350001, 3765613, 3224183, 3645522, 3779639, 3338447]\n",
      "1899999   362414.0                                     [3237379, 1913315, 1160587, 100245, 3096916, 3097620, 108860, 3530521, 2885725, 3759743, 634173, 3203594, 2466702]\n",
      "\n",
      "[1900000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "seq_df = pd.read_pickle(f'{preprocess_path}/ad_id_s64_total_seq.pkl')\n",
    "print(seq_df)\n",
    "seq_df = seq_df[seq_df.user_id < 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_csv(f'{data_path}/train_preliminary/user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = seq_df.merge(label_df,on='user_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 64\n",
    "emb_model = Word2Vec.load(f'model/ad_id_emb.model_{L}')\n",
    "print(emb_model)\n",
    "import numpy as np\n",
    "\n",
    "vocab_list = [word for word, Vocab in emb_model.wv.vocab.items()]# 存储 所有的 词语\n",
    "\n",
    "word_index = {\" \": 0} # 初始化 `[word : token]` ，后期 tokenize 语料库就是用该词典。使用前必须添加一个索引0.\n",
    "word_vector = {} # 初始化`[word : vector]`字典\n",
    "\n",
    "# 初始化存储所有向量的大矩阵，留意其中多一位（首行），词向量全为 0，用于 padding补零。\n",
    "# 行数 为 所有单词数+1 比如 10000+1 ； 列数为 词向量“维度”比如100。\n",
    "embedding_matrix = np.zeros((len(vocab_list) + 1, emb_model.vector_size))\n",
    "\n",
    "for i in range(len(vocab_list)):\n",
    "    # print(i)\n",
    "    word = vocab_list[i]  # 每个词语\n",
    "    word_index[word] = i + 1 # 词语：索引\n",
    "    word_vector[word] = emb_model.wv[word] # 词语：词向量\n",
    "    embedding_matrix[i + 1] = emb_model.wv[word]  # 词向量矩阵\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "hit=0\n",
    "miss=0\n",
    "for row in tqdm(total_df[['user_id','ad_id_seq']].values,total=len(total_df)):\n",
    "    try:\n",
    "        result.append([row[0],[word_index[i]  for i in row[-1]]])\n",
    "        hit+=1\n",
    "    except Exception as e:\n",
    "        miss+=1\n",
    "print(f'hit:{hit}, miss:{miss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_seq_df  = pd.DataFrame(result,columns=['user_id','ad_id_int_seq'])\n",
    "print(int_seq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df  = int_seq_df[int_seq_df.user_id <=720000]\n",
    "valid_df = int_seq_df[int_seq_df.user_id > 720000]\n",
    "\n",
    "train_df = train_df.merge(label_df,on='user_id',how='left')\n",
    "train_df['age'] =train_df['age'] -1\n",
    "\n",
    "valid_df = valid_df.merge(label_df,on='user_id',how='left')\n",
    "valid_df['age'] =valid_df['age'] -1\n",
    "\n",
    "\n",
    "train_x = np.array(train_df[['ad_id_int_seq']].values[:,0])\n",
    "train_y = train_df[['age']].values\n",
    "\n",
    "valid_x = np.array(valid_df[['ad_id_int_seq']].values[:,0])\n",
    "valid_y = valid_df[['age']].values\n",
    "\n",
    "before_one_hot =  train_y.reshape([-1,1])\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(before_one_hot)\n",
    "\n",
    "one_hoted_train_y  = enc.transform(before_one_hot).toarray()\n",
    "print(one_hoted_train_y.shape)\n",
    "\n",
    "before_one_hot =  valid_y.reshape([-1,1])\n",
    "print(before_one_hot)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(before_one_hot)\n",
    "\n",
    "one_hoted_valid_y  = enc.transform(before_one_hot).toarray()\n",
    "print(one_hoted_valid_y.shape)\n",
    "\n",
    "print(train_x)\n",
    "print(len(train_x))\n",
    "maxlen = 1000\n",
    "train_x = keras.preprocessing.sequence.pad_sequences(train_x, maxlen=maxlen)\n",
    "valid_x = keras.preprocessing.sequence.pad_sequences(valid_x, maxlen=maxlen)\n",
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_layer = Embedding(\n",
    "    len(vocab_list) +1,\n",
    "    emb_model.vector_size,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = embedding_layer(inputs)\n",
    "# Add 2 bidirectional LSTMs\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(one_hoted_train_y)\n",
    "print(valid_x.shape)\n",
    "print(one_hoted_valid_y)\n",
    "\n",
    "model.fit(train_x,one_hoted_train_y, validation_data=(valid_x,one_hoted_valid_y), epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
